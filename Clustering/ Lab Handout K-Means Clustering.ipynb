{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f341669-1fe9-4d1e-8a51-4e4dbe087058",
   "metadata": {},
   "source": [
    "# üß™ Lab Handout: K-Means Clustering\n",
    "\n",
    "\n",
    "In this lab, we will explore **K-Means Clustering**, an unsupervised learning algorithm used to group data into clusters.  \n",
    "We will use a sample dataset (`income.csv`) and visualize the results using scatter plots.\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives**:\n",
    "- Understand the steps of K-Means clustering\n",
    "- Implement K-Means using `scikit-learn`\n",
    "- Visualize clusters with Matplotlib / seaborn\n",
    "- Use the **Elbow Method** to find the optimal `k`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608447d9-4a45-47e6-8aa4-9e25105c4447",
   "metadata": {},
   "source": [
    "## üß© K-Means Clustering\n",
    "\n",
    "**K-means clustering** is a method to group data into clusters where each piece of data is closest to the central point, or **centroid**, of its cluster.\n",
    "\n",
    "### Steps of the K-Means Algorithm\n",
    "1. Start with **K centroids** by putting them at random places.  \n",
    "   Example: here K = 2 (randomly selected centroids).  \n",
    "2. Compute the **distance** of every point from each centroid and cluster them accordingly.  \n",
    "3. Adjust centroids so that they become the **center of gravity** for their respective clusters.  \n",
    "4. Re-cluster every point based on their updated distance from the centroids.  \n",
    "5. Again, adjust centroids.  \n",
    "6. Repeat steps until **data points stop changing clusters** (convergence).\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ **SSE** ‚Äì Sum of Squared Errors or **WCSS** - Within Clusters Sum of Square\n",
    "To find the **optimal number of clusters (K)** using SSE:\n",
    "- Plot the **sum of squared distances** from each data point to its cluster‚Äôs centroid.  \n",
    "- Then, select **K** where the decrease in SSE starts to **level off**, known as the **‚Äúelbow point.‚Äù**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c3b33-d720-4505-aeac-f0510438af65",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f55420-3137-42a3-8102-3c3c5fbb36d8",
   "metadata": {},
   "source": [
    "## [scikit-learn KMeans Reference](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n",
    "**from sklearn.cluster import KMeans**\n",
    "\n",
    "### ‚öôÔ∏è Key Parameters of `KMeans`\n",
    "\n",
    "| Parameter | Description |\n",
    "|------------|-------------|\n",
    "| **n_clusters** | Number of clusters (K) to form. |\n",
    "| **init** | Method for initializing centroids ‚Äî `'k-means++'` *(default)* or `'random'`. |\n",
    "| **n_init** | Number of times the algorithm runs with different centroid seeds (best result kept). |\n",
    "| **max_iter** | Maximum number of iterations for a single run. |\n",
    "| **random_state** | Controls random number generation for reproducibility. |\n",
    "---\n",
    "\n",
    "### üß© Main Methods\n",
    "\n",
    "| Method | Description |\n",
    "|---------|-------------|\n",
    "| **fit(X)** | Compute K-Means clustering on dataset `X`. |\n",
    "| **fit_predict(X)** | Fit model and return cluster labels for each data point. |\n",
    "| **predict(X)** | Assign new samples to the nearest existing cluster centroid. |\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Important Attributes\n",
    "\n",
    "| Attribute | Description |\n",
    "|------------|-------------|\n",
    "| **cluster_centers_** | Coordinates of the cluster centroids. |\n",
    "| **labels_** | Index (0, 1, 2, ‚Ä¶) of the cluster each data point belongs to. |\n",
    "| **inertia_** | Sum of squared distances (SSE) of samples to their nearest centroid. |\n",
    "\n",
    "---\n",
    "\n",
    "üìù **Note:**  \n",
    "- Always scale your data (e.g., using `StandardScaler`) before applying K-Means.  \n",
    "- Choose an appropriate `K` using the **Elbow Method**.  \n",
    "- K-Means assumes **spherical**, equally sized clusters and is sensitive to **outliers**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c56266-8256-42d5-81fe-511b8dca36c8",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f4da4-290e-4a02-bedd-9a9974865506",
   "metadata": {},
   "source": [
    "## Methods to determine optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d57542e-8218-4eda-b5f1-12072b5792c2",
   "metadata": {},
   "source": [
    "### 1. Elbow Method:\n",
    "\n",
    "The **Elbow Method** helps determine the **optimal number of clusters (K)** in a K-Means model.  \n",
    "It is based on analyzing the **Sum of Squared Errors (SSE)**, also known as **inertia** in scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Concept**\n",
    "\n",
    "- As K increases, the **SSE (inertia)** ‚Äî i.e., the sum of squared distances of samples to their nearest cluster center ‚Äî always **decreases**.  \n",
    "- Initially, this reduction is large, but after a certain K, the improvement slows down.  \n",
    "- The point where the **rate of decrease sharply changes** forms an **‚Äúelbow‚Äù** in the curve ‚Äî this K is usually a good choice.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Formula**\n",
    "\n",
    "The **SSE** (or inertia) is given by:\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=1}^{k} \\sum_{x_j \\in C_i} \\|x_j - \\mu_i\\|^2\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\( C_i \\) = cluster *i*  \n",
    "- \\( \\mu_i \\) = centroid of cluster *i*  \n",
    "- \\( x_j \\) = data points in cluster *i*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5989f9d-3e8a-4e58-8365-701240f78a3a",
   "metadata": {},
   "source": [
    "### 2. Silhouette Score \n",
    "\n",
    "The **Silhouette Score** is a metric that measures **how well data points fit within their clusters** in comparison to other clusters.  \n",
    "It helps evaluate the **quality of clustering** without knowing the true labels.\n",
    "\n",
    "---\n",
    "#### **Library**\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "#### **Formula**\n",
    "\n",
    "For each sample **i**:\n",
    "- **a(i)** = average distance between *i* and all other points in the same cluster.  \n",
    "- **b(i)** = smallest average distance of *i* to all points in any other cluster.  \n",
    "\n",
    "Then,  \n",
    "$$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
    "$$\n",
    "\n",
    "The overall **silhouette score** is the average of all `s(i)` values.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Score Range**\n",
    "\n",
    "| Silhouette Score | Interpretation |\n",
    "|------------------:|----------------|\n",
    "| **+1** | Perfectly clustered (well-separated clusters). |\n",
    "| **0** | Points are on or very close to the decision boundary. |\n",
    "| **-1** | Points are likely assigned to the wrong cluster. |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
